# LGMFN-code
Code for **LGMFN: A Local-Global Multimodal Fusion Network for Robust and Efficient Video Action Recognition**

# Abstract

LGMFN is a local-global multimodal fusion network that efficiently captures local and global movement patterns across modalities. It:
- Transforms unstructured RGB and depth action data into corresponding human local and global images, and simplifies complex and redundant video sequences into concise representations of local body movements and global motion.
- Achieves superior performance over state-of-the-art methods on the NTU RGB+D and NTU RGB+D 120 benchmarks through compact and efficient image-based classification and fusion.
- When fused with skeleton-based models, significantly improves the robustness of these baselines in distinguishing similar classes and achieving perspective invariance with little time overhead.

# Ablation studies

The detailed ablation studies on NTU RGB+D (CSub/CView) and NTU RGB+D 120 (XSub/XSet) are as follows(employing EfficientNet-b7 as baseline, and the Number is as in the paper).

| Number | Input |  Backbone | CSub(\%) | CView(\%) | XSub(\%) | XSet(\%) |
|--------|--------|--------|--------|--------|--------|--------|
| \#1    |  RL                | EfficientNetB7        | 91.7 | 95.9 | 88.4 | 88.9| 
| \#2    |  RG                | EfficientNetB7        | 89.8 | 95.2 | 87.2 | 87.8 |
| \#3    |  DL              | EfficientNetB7        | 88.5 | 88.6 | 85.6 | 85.6 |
| \#4    |  DG                | EfficientNetB7         | 90.1 | 90.2 | 84.4 | 86.7 |  
| \#5    |  RL + DL     | \#1+\#3         | 95.2 | 97.7 | 94.0 | 94.7 |  
| \#6    |  RG + DG                | \#2+\#4        | 95.2 | 97.8 | 93.5 | 94.6 |
| \#7    |  RL + RG                | \#1+\#2       | 95.1 | 98.2 | 93.0 | 93.9 |  
| \#8    |  DL + DG                | \#3+\#4       | 93.0 | 94.1 | 90.5 | 91.6 |  
| \#9    |  RL + DL + RG + DG                | \#1+\#2+\#3+\#4 (LGMFN)   | 96.8 | 98.9 | 95.9 | 96.8 |  
| \#10   |  $Skeleton\ Joints$              | MS-G3D        | 89.3 | 95.0 | 81.5 | 82.3 |
| \#11   |  $Skeleton\ Bones$              | MS-G3D        | 90.1 | 95.3 | 85.0 | 86.4 | 
| \#12   |  $Skeletons$              | \#10+\#11        | 91.5 | 96.2 | 86.3 | 87.4 | 
| \#13   |  $Skeleton\ Joints$              | PoseConv3D        | 93.7 | 96.5 | 85.9 | 89.7 |
| \#14   |  $Skeleton\ Bones$              | PoseConv3D        | 93.4 | 96.0 | 85.9 | 89.7 |
| \#15   |  $Skeletons$              | \#13+\#14        | 94.1 | 96.7 | 86.6 | 90.2 | 
| \#16   |  ${Skeletons}$ + RL + RG            | \#7+\#12 (EfficientNetB7 + MS-G3D)    | 97.3 | 99.4 | 95.6 | 96.5 | 
| \#17   |  ${Skeletons}$ + RL + RG            | \#7+\#15 (EfficientNetB7 + PoseConv3D)    | 97.0 | 99.1 | 94.5 | 95.8 | 
| \#18   |  ${Skeletons}$ + RL + RG + DL + DG            | \#9+\#12 (EfficientNetB7 + MS-G3D)    | 97.6 | 99.5 | 96.5 | 97.5 | 
| \#19   |  ${Skeletons}$ + RL + RG + DL + DG           | \#9+\#15 (EfficientNetB7 + PoseConv3D)    | 97.5 | 99.3 | 95.9 | 96.9 |

RL, RG, DL and DG denote RGB HLI, RGB HGI, Depth HLI and Depth HGI, respectively. $+$ indicates score fusion.

# Prerequisites

- Python;

- Pytorch;

- A small number of required packages can be installed using the pip install command.

- Each of the skeleton-based methods, [PoseConv3D](https://github.com/kennymckormick/pyskl/tree/main) and [MS-G3D(implemented via MMNet)](https://github.com/bruceyo/MMNet) requires a separate environment for setup. This step is optional and only needed for users who wish to reproduce the full pipeline.
 
 If you only need to quickly reproduce the experimental results in the article, please follow [Ensembling](#ensembling).

 # Data preparation and preprocess

 ## Dataset Access

To download the large-scale public datasets NTU RGB+D and NTU RGB+D 120, prior permission from [RoseLab](https://rose1.ntu.edu.sg/dataset/actionRecognition/) is required.

In this work, RGB, depth, and skeleton modalities are required.

## Preprocess

We provide HLI and HGI for depth maps and RGB videos for quick validation. Additionally, our code in `tools` allows you to generate HLI and HGI for different modalities as per your requirements.

For skeleton-based methods, the [3D heatmap volumes](https://github.com/kennymckormick/pyskl/blob/main/tools/data/README.md) are available for download in PoseConv3D, while [spatiotemporal skeleton graph](https://github.com/bruceyo/MMNet) can be generated by the source code of MS-G3D. If you wish to test the view-invariant property of the model, please change the 'training_cameras' setting in the 'ntu_gendata.py' file.

 **NTU RGB+D HLI and HGI**
 
 | Mid-level features | Quark Disk Link |
 |----------------|------------------|
 | RGB HLI         | uploading, coming soon |
 | RGB HGI| [Link](https://pan.quark.cn/s/ceb1cc695340?pwd=5Y29) |
 | Depth HLI       | [Link](https://pan.quark.cn/s/118190b09ce7?pwd=1V2W) |
 | Depth HGI       | regenerating, coming soon |

HLI or HGI images are stored in a single directory, regardless of benchmark splits. During code execution, each sample is identified as belonging to the training or testing set based on its filename (i.e., sample ID).

To specify the path to the saved HLI or HGI images, modify the rgb_images_path parameter in main.py.
 
 # Train and test model
 
 Each single-stream model is trained first, and then the learned model parameters are used for testing.
 
 Each data stream needs to be trained separately. For $Skeleton\ Joints$ and $Skeleton\ Bones$, please refer to the tutorial of PoseConv3D and MS-G3D. Here we introduce the training method about HLI and HGI.
 
 For RGB HLI and HGI:

 `python main.py`

We use the official code of [2s-AGCN](https://github.com/lshiwjx/2s-AGCN) or MS-G3D to generate labels for different benchmarks. You can also use these labels directly in the folder `data`.

**Pay attention to modify the file path.**

| File path parameter    | Description         |
|------------------------|---------------------|
| data_path              |  Label file - default='data'                 |
| dataset                |  Label file - Dataset, i.e., ntu or ntu120   |
| dataset_type           |  Label file - Benchmark, i.e., xsub or xview |
| output                 |  Output file        |
| rgb_images_path        |  RGB HLI or HGI file       |

 For depth HLI and HGI:

 `python main_depth.py`

In our code, EfficientNet is used as the default backbone for classifying HLI and HGI features. Due to file size limitations, the EfficientNet-B7 model file is not included in the repository. You can download it from [link](https://drive.google.com/drive/folders/1IzyAu7xu5jMRUuWpUe91rs6shw6kANGH?usp=sharing). 

After downloading, place the model file in the `weights` directory. In `model_rgb_efficientnet.py`, the following line sets the model path: `weights_path = '/weights/efficientnet-b7-dcc49843.pth'`. This path can be modified if needed. 

If you prefer to use ResNet as the backbone, simply change the following line in `main.py`: `from model_rgb_efficient import Model` to `from model_rgb import Model`

Additionally, when reproducing results on NTU RGB+D 120, make sure to update `num_class` in the same file to 120.


 # Ensembling
 
 Perform weighted score fusion. Here we use the highest score finally obtained by each modality single-stream input for fusion. It is worth noting that the best results are not necessarily obtained from the fusion of these highest scores. Please try it yourself.
 
 **You can quickly reproduce the experimental results in the article based on the content of this part only.**

 Due to the upload file size limit, we store ensemble-related files (final_results) in [link](https://drive.google.com/drive/folders/1LSwCh14o7Rg6BgWumoi8NeUVJDYAEOh3?usp=sharing).
 
 for results ensembling:

 `tools` - `python ensemble.py`
 
 You can change `alpha` to adjust the weights for different modalities.

# Acknowledgements

 This work is based on the following three worksï¼š

 **MMNet**, TPAMI 2022, [Original code](https://github.com/bruceyo/MMNet)
 
 **PoseConv3D**, CVPR 2022, [Original code](https://github.com/kennymckormick/pyskl)

 **MS-G3D**, CVPR 2020, [Original code](https://github.com/kenziyuliu/MS-G3D)

 Thanks to the original authors for their work! Although our work represents only a modest improvement upon existing studies, we remain optimistic that it can provide valuable enlightenment to someone.

 Meanwhile, we are very grateful to the creators of these two datasets, i.e., NTU RGB+D and NTU RGB+D 120. Your selfless work has made a great contribution to the computer vision community!

 Last but not least, the authors will be very grateful for the selfless and constructive suggestions of the reviewers and editoral team.
 
 
# Contact

If you find that the above description is not clear, or you have other issues that need to be communicated when conducting the experiment, please leave a message on Github.

Feel free to contact me via email:

    `Currently anonymous`
